{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# tf.enable_eager_execution()\n",
    "# tf.executing_eagerly()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import unicodedata \n",
    "import re\n",
    "import time\n",
    "import io\n",
    "from tensorflow import keras \n",
    "\n",
    "\n",
    "# tf.enable_eager_execution()\n",
    "# tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download data\n",
    "\n",
    "path_zip = keras.utils.get_file('spa-eng.zip' , \n",
    "                                origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "                                extract = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RKumarX0105498\\.keras\\datasets/spa-eng/spa.txt\n"
     ]
    }
   ],
   "source": [
    "path_to_file = os.path.dirname(path_zip)+\"/spa-eng/spa.txt\"\n",
    "print(path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def convert_unicode_file(s):\n",
    "#     return \"\".join(c for c in unicodedata.normalize('NED', s) \n",
    "#                    if unicodedata.category(c) != \"Mn\")\n",
    "\n",
    "    return \"\".join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != \"Mn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = convert_unicode_file(w.lower().strip())\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stack w = re.sub(r\"([?.!,多])\", r\" \\1 \", w)\n",
    "#     overflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "        \n",
    "    w = re.sub(r\"([?.!,多])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    \n",
    "     # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,多]+\", \" \", w)\n",
    "\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n",
      "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
     ]
    }
   ],
   "source": [
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"多Puedo tomar prestado este libro?\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "\n",
    "def create_dataset(path, no_example):\n",
    "    lines = io.open(path, encoding=\"UTF-8\").read().strip().split(\"\\n\")\n",
    "    \n",
    "    word_pair = [    [  preprocess_sentence(w) for w in l.split('\\t')  ]       for l in lines[:no_example]       ]\n",
    "    \n",
    "    return zip(*word_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "en, sp = create_dataset(path_to_file, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
      "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
     ]
    }
   ],
   "source": [
    "print(en[-1])\n",
    "print(sp[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_tensor(tensor):\n",
    "    return max(len(m)  for m in tensor )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(lang):\n",
    "    long_tokenizer = keras.preprocessing.text.Tokenizer(filters=\"\")\n",
    "    long_tokenizer.fit_on_texts(lang)\n",
    "    tensor = long_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding=\"post\")\n",
    "    return tensor, long_tokenizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, no_example = None):\n",
    "    #create clean input, output pair\n",
    "    tar_lang, inp_lang = create_dataset(path, no_example)\n",
    "    inp_tensor, inp_long_tokenizer = tokenizer(inp_lang)\n",
    "    tar_tensor, tar_long_tokenizer = tokenizer(tar_lang)\n",
    "    \n",
    "    return inp_tensor, tar_tensor, inp_long_tokenizer, tar_long_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1  135    3 ...    0    0    0]\n",
      " [   1  293    3 ...    0    0    0]\n",
      " [   1  595    3 ...    0    0    0]\n",
      " ...\n",
      " [   1   18 9413 ...    0    0    0]\n",
      " [   1   63 2490 ...    0    0    0]\n",
      " [   1   23 2175 ...    0    0    0]]\n",
      "[[ 1 36  3 ...  0  0  0]\n",
      " [ 1 36  3 ...  0  0  0]\n",
      " [ 1 36  3 ...  0  0  0]\n",
      " ...\n",
      " [ 1 16 38 ...  0  0  0]\n",
      " [ 1 16 38 ...  0  0  0]\n",
      " [ 1 16 38 ...  0  0  0]]\n",
      "<keras_preprocessing.text.Tokenizer object at 0x0000023436335198>\n",
      "<keras_preprocessing.text.Tokenizer object at 0x0000023431F3A320>\n"
     ]
    }
   ],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "\n",
    "no_example = 30000\n",
    "inp_tensor, tar_tensor, inp_long_tokenizer, tar_long_tokenizer = load_dataset(path_to_file , no_example)\n",
    "\n",
    "#calculate maximum length of target tensor\n",
    "max_len_tar, max_len_inp = max_tensor(tar_tensor) , max_tensor(inp_tensor)\n",
    "\n",
    "\n",
    "print(inp_tensor)\n",
    "print(tar_tensor)\n",
    "print(inp_long_tokenizer)\n",
    "print(tar_long_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "print(max_len_tar)\n",
    "print(max_len_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000 24000 6000 6000\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "# input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(inp_tensor, tar_tensor, test_size=0.2)\n",
    "\n",
    "# x_train, y_train, x_test, y_test = train_test_split(inp_tensor, tar_tensor, test_size=0.2)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(inp_tensor, tar_tensor, test_size = 0.2)\n",
    "\n",
    "# Show length\n",
    "# print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))\n",
    "print(len(x_train),len(y_train), len(x_test),  len(y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "13 ----> la\n",
      "7161 ----> ignorancia\n",
      "7 ----> es\n",
      "21 ----> una\n",
      "7162 ----> bendicion\n",
      "3 ----> .\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "3910 ----> ignorance\n",
      "8 ----> is\n",
      "3911 ----> bliss\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_long_tokenizer, x_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(tar_long_tokenizer, y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ((16,), (11,)), types: (tf.int32, tf.int32)>\n",
      "<DatasetV1Adapter shapes: ((64, 16), (64, 11)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = len(x_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(x_train) // BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_input_size = len(inp_long_tokenizer.word_index) + 1\n",
    "vocab_target_size = len(tar_long_tokenizer.word_index) + 1\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(BUFFER_SIZE)\n",
    "print(dataset)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder = True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "__iter__() is only supported inside of tf.function or when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-2efe5b9f25b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# example_input_batch, example_target_batch = next(iter(dataset))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mexample_input_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexample_target_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2032\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2033\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2034\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2035\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2036\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    341\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIteratorV2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n\u001b[0m\u001b[0;32m    344\u001b[0m                          \"or when eager execution is enabled.\")\n\u001b[0;32m    345\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: __iter__() is only supported inside of tf.function or when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "# example_input_batch, example_target_batch = next(iter(dataset))\n",
    "\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
