{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import ExtraTreeRegressor # why?\n",
    "\n",
    "# import gym # why use gym ?????\n",
    "\n",
    "from scipy import stats as s\n",
    "from random import randint\n",
    "from re import search\n",
    "from pathlib import Path\n",
    "\n",
    "import glob\n",
    "from functools import reduce\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import csv\n",
    "\n",
    "# import pprint\n",
    "from pprint import pprint \n",
    "\n",
    "# from pandas_profiling import ProfileReport\n",
    "\n",
    "#ProfileReport(df) # get all details like : min, max, correlation, else\n",
    "\n",
    "# %matplotlib notebook vs %matplotlib inline #vvi : %matplotlib notebook\n",
    "\n",
    "\n",
    "from scipy.stats import mstats\n",
    "\n",
    "# df=pd.read_csv(\"CarPrice_Assignment.csv\",encoding = \"ISO-8859-1\",low_memory=False)\n",
    "\n",
    " #************ titanic_df.profile_report() #VVI \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='green' > Common\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "from collections import Counter , defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "from pandas import Series as s , DataFrame as df\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# To ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from matplotlib import pyplot as plt, rcParams as rc\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "rc[\"figure.figsize\"] = 10,6\n",
    "\n",
    "import datetime\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.model_selection  import StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from iteration_utilities import duplicates, unique_everseen\n",
    "\n",
    "import sys\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from timeit import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='red' > Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "#preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "\n",
    "from xgboost import XGBRegressor , XGBRFRegressor\n",
    "\n",
    "import lightgbm\n",
    "\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# GridSearchCV to find optimal min_samples_leaf\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='orange' > Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Algorithm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "# from catboost import CatBoostClassifier\n",
    "\n",
    "#preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Metrics\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, recall_score, precision_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_recall_fscore_support\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# GridSearchCV to find optimal min_samples_leaf\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='blue' > HighLevel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from imblearn.combine import SMOTETomek # over sampling method 1\n",
    "\n",
    "## RandomOverSampler to handle imbalanced data\n",
    "from imblearn.over_sampling import RandomOverSampler # over sampling method 2\n",
    "\n",
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.feature_selection import RFE\n",
    "import statsmodels.api as sm \n",
    "\n",
    "\n",
    "from pickle import dump, load\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='red' > Deep Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='yellow' > NLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data Set\n",
    "telecom_df =  pd.read_csv('telecom_churn_data.csv')\n",
    "\n",
    "pd.options.display.max_columns = 250\n",
    "pd.options.display.max_rows = 2000\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get only duplicate value :  [1, 1, 2, 2]\n",
      "all unique value         :  [1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "l1 = [1,1,2,1,2,3,4,2]\n",
    "def get_only_duplicate_list_value(l):\n",
    "    return list(duplicates(l))\n",
    "\n",
    "def get_only_unique_list_value(l):\n",
    "    return list(unique_everseen(l))\n",
    "\n",
    "\n",
    "print(\"get only duplicate value : \", get_only_duplicate_list_value(l1))\n",
    "\n",
    "\n",
    "print(\"all unique value         : \", get_only_unique_list_value(l1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute for missing value(Numberical : mean, categorical :mode(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataFrameImputer(TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Impute missing values.\n",
    "\n",
    "        Columns of dtype object are imputed with the most frequent value \n",
    "        in column.\n",
    "\n",
    "        Columns of other types are imputed with mean of column.\n",
    "\n",
    "        \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0] if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],\n",
    "            index=X.columns)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)\n",
    "    \n",
    "df6 = DataFrameImputer().fit_transform(df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import is_numeric_dtype\n",
    "def remove_outlier(dataframe):\n",
    "    low = .05\n",
    "    high = .95\n",
    "    quant_df = dataframe.quantile([low, high])\n",
    "    for name in list(dataframe.columns):\n",
    "        if is_numeric_dtype(dataframe[name]):\n",
    "            dataframe = dataframe[(dataframe[name] > quant_df.loc[low, name]) & (dataframe[name] < quant_df.loc[high, name])]\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorical_col_name_df(dataset): #testing has been pending\n",
    "    return dataset.select_dtypes(include=['object']) #get all categorical columns only\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentences = [\n",
    "    'i love my dog',\n",
    "    'I, love my cat',\n",
    "    'You love my dog!',\n",
    "    \"I am running with dog\"\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 1)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seeking only the numeric features from the data or selecting only categorical column\n",
    "numeric_features = df2.select_dtypes(include = [np.number] or [\"object\"] or [np.float])\n",
    "\n",
    "df['ExterQual'] = df.ExterQual.map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic Funtions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='green'> Checking : How much Missing values present in percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def understanding_dataset(dataset):\n",
    "    print(f\"Shape: {dataset.shape}\")\n",
    "    print(f\"Total Missing Value in Dataset: {dataset.isna().sum().sum()}\")\n",
    "    \n",
    "    for column in dataset.columns:\n",
    "        print(f\"===============Column: {column} ==============\")\n",
    "        print(f\"Number of unique values: {dataset[column].nunique()}\")\n",
    "        print(f\"Max: {dataset[column].max()}\")\n",
    "        print(f\"Min: {dataset[column].min()}\")\n",
    "        \n",
    "        if(dataset[column].isna().any()):\n",
    "            print(f\"Missing Value: {round((data[[column]].isna().sum() / len(data) ) * 100 , 2)}\")\n",
    "    \n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "def get_percentage_miss_value(dataset):\n",
    "    higher_miss_value_column = []\n",
    "    miss_threshold_value = 50\n",
    "    \n",
    "    for i in dataset.columns:\n",
    "        if dataset[i].isna().sum() > 1: \n",
    "            perectange_val = (dataset[i].isna().sum() / len(dataset)) * 100\n",
    "            print(\"Column-> \" , i, \", total no of missing value : \",dataset[i].isna().sum() , \" & :         \", round(perectange_val,2) ,\" %\")\n",
    "                \n",
    "            if(perectange_val > miss_threshold_value):\n",
    "                higher_miss_value_column.append(i)\n",
    "            \n",
    "    print(\"\\n\\n\\n\")\n",
    "    \n",
    "    if higher_miss_value_column:\n",
    "        print(\"Higher Missing values in Columns for Delete : \", higher_miss_value_column)\n",
    "    else:\n",
    "        print(\"There are no Higher Column Missing values in Dataset\")\n",
    "        \n",
    "\n",
    "def check_cloumn_details_type_categorical(dataset):\n",
    "    for i in dataset.columns:\n",
    "        if(dataset[i].dtype == \"object\"):\n",
    "            print(\"Columns name :  \",i  )\n",
    "            \n",
    "            print(dict(Counter(dataset[i])))\n",
    "            \n",
    "            print(\"*\"*100)\n",
    "            print(\"\\n\")\n",
    "\n",
    "def check_cloumn_details_type_numberical(dataset):\n",
    "    for i in dataset.columns:\n",
    "        if (dataset[i].dtype == \"int\"):\n",
    "            print(\"Columns name :  \",i  )\n",
    "            \n",
    "            print(dict(Counter(dataset[i])))\n",
    "            print(\"*\"*100)\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            \n",
    "def check_cloumn_details_type_float(dataset):\n",
    "    for i in dataset.columns:\n",
    "        if (dataset[i].dtype == \"float\"):\n",
    "            print(\"Columns name :  \",i  )\n",
    "            \n",
    "            print(dict(Counter(dataset[i])))\n",
    "            print(\"*\"*100)\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            \n",
    "def convet_categorical_numerical_use_get_dummy(dataset):\n",
    "    df2_categorical = dataset.select_dtypes(include =  [\"object\"] )\n",
    "    df_dummies = pd.get_dummies(df2_categorical,drop_first=True)\n",
    "    \n",
    "    # drop categorical variables \n",
    "    dataset = dataset.drop(columns=list(df2_categorical.columns), axis=1)\n",
    "    \n",
    "    # concat dummy variables with X\n",
    "    dataset = pd.concat([dataset, df_dummies], axis=1)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "    \n",
    "def get_column_integer_float_list(dataset):\n",
    "    numberical_int_columns = []\n",
    "    for i in df2.columns:\n",
    "        if df2[i].dtype == \"int\":\n",
    "            numberical_int_columns.append(i)\n",
    "        \n",
    "    numberical_float_columns = []\n",
    "    for i in df2.columns:\n",
    "        if df2[i].dtype == \"float\":\n",
    "            numberical_float_columns.append(i)\n",
    "            \n",
    "    return numberical_int_columns, numberical_float_columns\n",
    "\n",
    "# pair_plot(numberical_int_columns[0:3] or numberical_float_columns[0:3])\n",
    "\n",
    "\n",
    "\n",
    "# We one-hot encode the categorical features\n",
    "def one_hot_encoding(rides):\n",
    "    dummy_fields = ['season', 'weather', 'month', 'hour', 'weekday']\n",
    "    for each in dummy_fields:\n",
    "        dummies = pd.get_dummies(rides[each], prefix=each, drop_first=False)\n",
    "        rides = pd.concat([rides, dummies], axis=1)\n",
    "    return rides\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_outliers(dataset, columns_name):\n",
    "    for chk in columns_name:\n",
    "        \n",
    "#         plt.boxplot(dataset[chk]) # please with graph & it is optional\n",
    "        \n",
    "        Q1 = dataset[chk].quantile(.25)\n",
    "        Q3 = dataset[chk].quantile(.75)\n",
    "        IQR = Q3-Q1\n",
    "        dataset =  dataset[(dataset[chk] >= (Q1-(1.5*IQR))) & (dataset[chk] <= (Q3+(1.5*IQR)))] \n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='red' > Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visulazing the distibution of the data for every feature\n",
    "def visualize_hist(dataset):\n",
    "    dataset.hist(edgecolor='black', linewidth=1.2, figsize=(20, 20));\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='green' > Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardscaler_preprocessing(dataset_train, dataset_test, num_col):\n",
    "    scaler = StandardScaler()\n",
    "   \n",
    "    dataset_train[num_col] = scaler.fit_transform(dataset_train[num_col])\n",
    "\n",
    "    dataset_test[num_col] = scaler.transform(dataset_test[num_col])\n",
    "    \n",
    "    return dataset_train, dataset_test\n",
    "\n",
    "\n",
    "#Label encoding\n",
    "def convert_to_numerical_datatype_train(dataset):\n",
    "    enc = LabelEncoder()\n",
    "    for i in dataset.columns:\n",
    "        if(dataset[i].dtype == \"object\"):\n",
    "            dataset[i] = enc.fit_transform(dataset[i])\n",
    "            \n",
    "    return dataset\n",
    "\n",
    "def visualize_histogram(dataset):\n",
    "    # plot histogram\n",
    "    plt.figure(figsize=(25, 9))  # figure size in ratio 16:9\n",
    "    features = dataset.columns  # list of columns name\n",
    "    for i, j in enumerate(features):\n",
    "        plt.subplot(3, 3, i + 1)  # create subplot for histogram\n",
    "        plt.title(\"Histogram of {}\".format(j), fontsize=15)  # title of histogram\n",
    "\n",
    "        bins = len(dataset[j].unique())  # bins for histogram\n",
    "        plt.hist(dataset[j], bins=bins, rwidth=0.8, edgecolor=\"y\", linewidth=2, )  # plot histogram\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.5)  # space between horixontal axes (subplots)\n",
    "    \n",
    "def corr_metrix(dataset):\n",
    "    corr = dataset.corr()\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap='RdYlGn')\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5});\n",
    "    ax.tick_params(labelsize=20)\n",
    "\n",
    "def corr_2_more_visualize(dataset):\n",
    "    corr = dataset.corr()\n",
    "    \n",
    "    f, ax = plt.subplots(figsize=(20, 9))\n",
    "    sns.heatmap(corr.apply(lambda x : np.round(x,2)), \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values,annot=True,cmap='RdYlGn', annot_kws={\"size\": 15})\n",
    "    ax.tick_params(labelsize=20)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def linear_regression_train_test():\n",
    "    #Linear regression with L2 regularization\n",
    "    for i in range(-2, 3):\n",
    "        alpha = 10**i\n",
    "        rm = Ridge(alpha = alpha)\n",
    "        ridge_model = rm.fit(X_train, y_train)\n",
    "        preds_ridge = ridge_model.predict(X_test)\n",
    "    \n",
    "        plt.scatter(preds_ridge, y_test, alpha= 0.75, c= 'b')\n",
    "        plt.xlabel('Predicted price')\n",
    "        plt.ylabel('Actual price')\n",
    "        plt.title('Ridge redularization with alpha {}'.format(alpha))\n",
    "        overlay = 'R square: {} \\nRMSE: {}'.format(ridge_model.score(X_test, y_test), mean_squared_error(y_test, preds_ridge))\n",
    "        plt.annotate(s = overlay, xy = (12.1, 10.6), size = 'x-large')\n",
    "        plt.show()\n",
    "        \n",
    "def actual_predict_visualization(actual_values, predict_values):\n",
    "    plt.scatter(actual_values, predict_values, alpha= 0.75, color = 'b')\n",
    "\n",
    "    plt.xlabel('Predicted price')\n",
    "    plt.ylabel('Actual price')\n",
    "    plt.title('Regression Model')\n",
    "    plt.show()\n",
    "    \n",
    "def visualize_original_predict_test(X_val, Y_val, default = \"Test\"):\n",
    "    print(\"Dataset type  : \\n\" ,default)\n",
    "    plt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\n",
    "    plt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    \n",
    "    plt.xlabel(\"X axis\", color = \"red\")\n",
    "    plt.ylabel(\"Y axis\", color = \"green\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def visualize_categorical_values(dataset):\n",
    "    no_of_columns = 4\n",
    "    no_of_rows = 4\n",
    "    \n",
    "    columns_object_type = [i for i in dataset.columns  if dataset[i].dtype == \"object\"]\n",
    "    total_rows = (len(columns_object_type) // no_of_rows ) + 1\n",
    "    \n",
    "    f, axes = plt.subplots(total_rows, no_of_columns, figsize=(18,24))\n",
    "\n",
    "    for ind, val in enumerate(columns_object_type):\n",
    "        sns.countplot(df[val] , ax = axes[ind // no_of_rows , ind %no_of_columns ])\n",
    "    plt.show()   \n",
    "    \n",
    "sns.pairplot(data = telecom_df[['arpu_6','arpu_7','arpu_8','churn_flag']],hue = 'churn_flag')\n",
    "\n",
    "def visualize_pairplot_target(numberical_col_list , target_val):\n",
    "    plt.figure(figsize=(18,34))\n",
    "    sns.pairplot(data = telecom_df[numberical_col_list],hue = target_val)\n",
    "    \n",
    "    #     sns.pairplot(data = telecom_df[['arpu_6','arpu_7','arpu_8','churn_flag']],hue = 'churn_flag') \n",
    "    #check for classification\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def visualize_numberical_values(dataset):\n",
    "    plt.figure(figsize=(18,34))\n",
    "    sns.pairplot(df)\n",
    "    plt.show()\n",
    "# visualize_numberical_values(df)\n",
    "    \n",
    "def check_skewness_numerical(dataset, target):\n",
    "    #analysing the distribution of sale price\n",
    "    print('skew is', dataset.SalePrice.skew())   \n",
    "    plt.hist(dataset[target], color= 'b')\n",
    "\n",
    "    plt.title('Distribution of ' + target, fontsize = 24)\n",
    "    plt.ylabel('observation', fontsize = 20)\n",
    "    plt.xlabel(target, fontsize = 20)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def skewness_after_log_transform(dataset , target):\n",
    "    #log transforming sale price to transform it into gaussian distribution\n",
    "    target1 = np.log(dataset.target)\n",
    "    print('skew is', target1.skew())\n",
    "    plt.hist(target, color= 'b')\n",
    "\n",
    "    plt.title('Distribution of ' + target, fontsize = 24)\n",
    "    plt.ylabel('observation', fontsize = 20)\n",
    "    plt.xlabel(target , fontsize = 20)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "# Since pairplots for all numeric values together are not clear we can make groups ,do plot with price & analyse\n",
    "\n",
    "def pair_plot(list_4_numberical_values):\n",
    "    sns.pairplot(df2, x_vars= list_4_numberical_values, y_vars='SalePrice',size=4, kind='scatter')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='red' > Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows=[]\n",
    "def addRandomStateForAlgorithm(x,y,names,algorithms,columns_name,random_state_list):    \n",
    "    for j in range(len(algorithms)):\n",
    "        model = algorithms[j]\n",
    "        for i in random_state_list:\n",
    "            x_train, x_test , y_train , y_test = train_test_split(x ,y , test_size = 0.25 , random_state = i)\n",
    "            model.fit(x_train,y_train)\n",
    "            pred_test = model.predict(x_test)\n",
    "            row = [names[j],i,r2_score (y_test,pred_test)]\n",
    "            rows.append(row)\n",
    "    models_df = pd.DataFrame(rows)   \n",
    "    models_df.columns = columns_name\n",
    "    print(models_df)\n",
    "    \n",
    "    \n",
    "def mean_absolute_percentage_error(y_true, y_pred) :\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    return np.mean(np.abs( (y_true - y_pred) / 100) ) * 100\n",
    "\n",
    "\n",
    "def root_mean_sequare_error(y_true, y_pred) : \n",
    "    mse = mean_squared_error(y_true,  y_pred)  \n",
    "    return np.sqrt(mse)\n",
    "   \n",
    "\n",
    "rows=[]\n",
    "\n",
    "def addRandomStateForAlgorithm(x,y,names,algorithms,columns_name,random_state_list):    \n",
    "    for j in range(len(algorithms)):\n",
    "        model = algorithms[j]\n",
    "        for i in random_state_list:\n",
    "            \n",
    "            x_train, x_test , y_train , y_test = train_test_split(x ,y , test_size = 0.30 , random_state = i)\n",
    "            \n",
    "            model.fit(x_train,y_train)\n",
    "            \n",
    "            y_pred_train = model.predict(x_train)\n",
    "            y_pred = model.predict(x_test)\n",
    "            \n",
    "            train_acc = r2_score(y_train, y_pred_train)\n",
    "            train_acc = round(train_acc, 2) * 100\n",
    "            \n",
    "            test_acc = r2_score(y_test, y_pred)\n",
    "            test_acc = round(test_acc, 2) * 100\n",
    "            \n",
    "            mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "            mape = round(mape, 2)\n",
    "            \n",
    "            mae = mean_absolute_error(y_test, y_pred)   \n",
    "            mae = round(mae, 2)\n",
    "            \n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            mse = round(mse, 2)\n",
    "            \n",
    "            rmse = root_mean_sequare_error(y_test, y_pred)\n",
    "            rmse = round(rmse, 2)\n",
    "\n",
    "            row = [names[j],   i,   train_acc, test_acc,  mae,    mse]\n",
    "    \n",
    "            rows.append(row)\n",
    "            \n",
    "    models_df = pd.DataFrame(rows) \n",
    "    \n",
    "    models_df.columns = columns_name\n",
    "    print(models_df)\n",
    "    \n",
    "names_regression = [ \"Lasso\", \"Ridge\"]\n",
    "algorithms = [ Lasso(), Ridge(alpha=ridge_alpha)]\n",
    "\n",
    "columns_name = [\"Model\",    \"Random_state\",   'Train_acc',     \"Test_acc\",   \"MAE\",   \"MSE\"]\n",
    "\n",
    "random_state_list_up_to_10 = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "\n",
    "addRandomStateForAlgorithm(X,y,names_regression,algorithms,columns_name,random_state_list_up_to_10)\n",
    "\n",
    "\n",
    "def get_accuracy(y_actual, y_pred, data_type = \"Train\"):\n",
    "    print(\"Model Evaluation Dataset Type is  : \", data_type)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"Accuracy  :  \", r2_score(y_actual, y_pred))\n",
    "    print(\"MAE       :  \", mean_absolute_error(y_actual, y_pred))\n",
    "    print(\"MSLE      :  \", mean_squared_log_error(y_actual, y_pred))\n",
    "    print(\"MSE       :  \", mean_squared_error(y_actual, y_pred))\n",
    "    print(\"RMSE      :  \", np.sqrt(mean_squared_error(y_actual, y_pred)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='green' > Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_train_val , y_pred_val , dataset_type = \"Default\"):\n",
    "    \n",
    "    print(\" Dataset type is : \", dataset_type)\n",
    "    \n",
    "    print(\"\\n Accuracy Score     : \",round(accuracy_score(y_train_val, y_pred_val), 4) * 100)\n",
    "    \n",
    "    print(\"\\n precision_accuracy : \",round(precision_score(y_train_val, y_pred_val), 4) * 100)\n",
    " \n",
    "    print(\"\\n recall_accuracy    : \",round(recall_score(y_train_val, y_pred_val), 4) * 100)\n",
    "    \n",
    "    print(\"\\n roc_auc_accuracy   : \",round(roc_auc_score(y_train_val, y_pred_val), 4) * 100)\n",
    "    \n",
    "    print(\"\\n f1_score_accuracy  : \",round(f1_score(y_train_val, y_pred_val), 4) * 100)\n",
    "    \n",
    "    print(\"\\n explained_variance  : \",round(explained_variance_score(y_train_val, y_pred_val), 4) * 100)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_train_val, y_pred_val).ravel()\n",
    "    \n",
    "    print(\"\\n Confusion Matrix TN : \", tn, \" FP : \", fp, \" FN : \", fn, \" TP : \", tp)\n",
    "\n",
    "    \n",
    "rows=[]\n",
    "def addRandomStateForAlgorithm(x,y,names,algorithms,columns_name,random_state_list):    \n",
    "    for j in range(len(algorithms)):\n",
    "        model = algorithms[j]\n",
    "        for i in random_state_list:\n",
    "            x_train, x_test , y_train , y_test = train_test_split(x ,y , test_size = 0.25 , random_state = i)\n",
    "            model.fit(x_train,y_train)\n",
    "            pred_test = model.predict(x_test)\n",
    "            row = [names[j],i,r2_score (y_test,pred_test)]\n",
    "            rows.append(row)\n",
    "    models_df = pd.DataFrame(rows)   \n",
    "    models_df.columns = columns_name\n",
    "    print(models_df)\n",
    "   \n",
    "\n",
    "rows=[]\n",
    "\n",
    "def addRandomStateForAlgorithm(x,y,names,algorithms,columns_name,random_state_list):    \n",
    "    for j in range(len(algorithms)):\n",
    "        model = algorithms[j]\n",
    "        for i in random_state_list:\n",
    "            \n",
    "            x_train, x_test , y_train , y_test = train_test_split(x ,y , test_size = 0.30 , random_state = i)\n",
    "            \n",
    "            model.fit(x_train,y_train)\n",
    "            \n",
    "            y_pred_train = model.predict(x_train)\n",
    "            y_pred = model.predict(x_test)\n",
    "            \n",
    "            train_acc = accuracy_score(y_train, y_pred_train)\n",
    "            train_acc = round(train_acc, 4) * 100\n",
    "            \n",
    "            test_acc = accuracy_score(y_test, y_pred)\n",
    "            test_acc = round(test_acc, 4) * 100\n",
    "            \n",
    "            roc_auc_score_acc = roc_auc_score(y_test, y_pred)\n",
    "            roc_auc_score_acc = round(roc_auc_score_acc, 4) * 100\n",
    "            \n",
    "\n",
    "            row = [names[j],   i,   train_acc, test_acc, roc_auc_score_acc]\n",
    "    \n",
    "            rows.append(row)\n",
    "            \n",
    "    models_df = pd.DataFrame(rows) \n",
    "    \n",
    "    models_df.columns = columns_name\n",
    "    print(models_df)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "names_regression = [ \"LightGBM\", \"RF\", \"XGBoost\" , \"SVM\"]\n",
    "algorithms = [ LGBMClassifier(), RandomForestClassifier(), XGBClassifier(), SVC()]\n",
    "\n",
    "columns_name = [\"Model\",    \"Random_state\",   'Train_acc',     \"Test_acc\" , \"roc_auc_score\"]\n",
    "\n",
    "random_state_list_up_to_10 = [1,2,3]\n",
    "\n",
    "\n",
    "addRandomStateForAlgorithm(X,Y,names_regression,algorithms,columns_name,random_state_list_up_to_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='green' > Imabalnced Dataset : Over Sampling(Majoruity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_imbalanced_dataset(dataset, target_col):\n",
    "    feature_columns = df5.columns.tolist()\n",
    "    feature_columns = [c for c in feature_columns if c not in [target_col]]\n",
    "\n",
    "    X2 = df5[feature_columns]\n",
    "    Y2 = df5[target_col]\n",
    "\n",
    "    os =  RandomOverSampler(random_state=35)\n",
    "    X_feature_variables , y_output = os.fit_sample(X2, Y2)\n",
    "    \n",
    "    return X_feature_variables , y_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='green' > XGBoost Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "\n",
    "model_final.fit(X, Y)\n",
    "\n",
    "y_pred_final = model_final.predict(df5_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso()\n",
    "\n",
    "# list of alphas to tune\n",
    "params = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n",
    " 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n",
    " 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n",
    "\n",
    "# cross validation\n",
    "folds = 5\n",
    "\n",
    "# cross validation\n",
    "model_cv = GridSearchCV(estimator = lasso, \n",
    "                        param_grid = params, \n",
    "                        scoring= 'neg_mean_absolute_error', \n",
    "                        cv = folds, \n",
    "                        return_train_score=True,\n",
    "                        verbose = 1)            \n",
    "\n",
    "model_cv.fit(X_train, y_train) \n",
    "\n",
    "\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "cv_results.head()\n",
    "\n",
    "\n",
    "\n",
    "# plotting mean test and train scoes with alpha \n",
    "cv_results['param_alpha'] = cv_results['param_alpha'].astype('int32')\n",
    "\n",
    "# plotting\n",
    "plt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\n",
    "plt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('Negative Mean Absolute Error')\n",
    "plt.title(\"Negative Mean Absolute Error and alpha\")\n",
    "plt.legend(['train score', 'test score'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#plz check for regression also? check for train dataset also?\n",
    "def check_cross_validation_overfit(X_val, Y_val, classification_model):\n",
    "    #cv = use diff\n",
    "    cross_validation_accuracy = cross_val_score(estimator = X_val, X = X_val, y = Y_val, cv = 2).mean()\n",
    "    \n",
    "    model_accuracy =  accuracy_score(X_val,   Y_val)\n",
    "    \n",
    "    print(\"checking overfit & underfit :  almost same\" )\n",
    "\n",
    "    print(\"Model Accuracy : \", model_accuracy, \" & cross validation \", cross_validation_accuracy )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='green' > LightBoost Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X ,Y , test_size = 0.05 , random_state = 10)\n",
    "\n",
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    \n",
    "    'metric': {'l2', 'l1'},\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "print('Starting training...')\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=20,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=5)\n",
    "\n",
    "y_pred_final = gbm.predict(df5_test, num_iteration=gbm.best_iteration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='red' > LightBoost Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'l2', 'l1'},\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "print('Starting training...')\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=20,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=5)\n",
    "\n",
    "print('Saving model...')\n",
    "# save model to file\n",
    "gbm.save_model('model.txt')\n",
    "\n",
    "print('Starting predicting...')\n",
    "# predict\n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "# eval\n",
    "print('The rmse of prediction is:', mean_squared_error(y_test, y_pred) ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "L1 = [1,2,45,55,5,4,4,4,4,4,4,5456,56,6,7,67]\n",
    "\n",
    "def max_occurrences_1a(seq=L):\n",
    "    \"dict iteritems\"\n",
    "    c = dict()\n",
    "    for item in seq:\n",
    "        c[item] = c.get(item, 0) + 1\n",
    "    return max(c.iteritems(), key=itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_occurrences_1a(L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_occurrences_3a(seq=L):\n",
    "    \"sort groupby generator expression\"\n",
    "    return max(((k, sum(1 for i in g)) for k, g in groupby(sorted(seq))), key=itemgetter(1))\n",
    "max_occurrences_3a(L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    ">>> L = [1, 2, 45, 55, 5, 4, 4, 4, 4, 4, 4, 5456, 56, 6, 7, 67] \n",
    ">>> statistics.mode(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [1,2,3,2,3,2,3]\n",
    "statistics.mode(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on Key = 0, based on value = 1 and based on Ascending = False or Descending = True\n",
    "#Default based on value and ascending\n",
    "\n",
    "def reverse_dict(dict_val , key_value = 1, ascending_descending = False):\n",
    "    \n",
    "    val = sorted(dict_val.items(), key = lambda x: x[key_value] , reverse = ascending_descending)\n",
    "    \n",
    "    return dict(val)\n",
    "\n",
    "def no_of_item_dict(dict_val , no_of_item):\n",
    "    dict_len = len(dict_val)\n",
    "    \n",
    "    if(dict_len != 0 and dict_len != 0 and  no_of_item <= dict_len):\n",
    "        for ind, val in enumerate(dict_val):\n",
    "            if(ind < no_of_item):\n",
    "                print(val[0] , \" \", val[1])                \n",
    "    else:\n",
    "        print(\"Please enter the valid input\")\n",
    "\n",
    "\n",
    "marks = {\"Raka\" : 99, \"ananya\" : 90 ,'John':85, 'Alex':80, 'Richard': 87, \"rakesh\" : 97 , \"Trishala\" : 92}\n",
    "dic_sorted = reverse_dict(marks, 0, False)\n",
    "\n",
    "print(dic_sorted)\n",
    "\n",
    "no_of_item_dict(dic_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra VVI Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.to_list() or list(df.columns) or list(df.iloc[:,0:].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [f for f in os.listdir('.') if os.path.isfile(f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MAE  : %f\\nMSE  : %f\\nMSLE : %f\\nRMSE : %F\"%(MAE, MSE, MSLE, RMSE)) # %f or %F"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
